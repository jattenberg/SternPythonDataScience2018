{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting IMDB movie review polarity\n",
    "\n",
    "Sentiment analysis is a hot topic in data science right now due to the immense amount of user-generated text data being created every day online.  Businesses can now look at what is being said about them on review sites to get an idea of how well they are liked, how much they are disliked, and what they can do to improve.  While most of this data is unlabeled, some sites also ask users to provide a numerical or star rating.  This allows us to build a classifier for positive/negative reviews using the star rating as a label, which could then hypothetically applied to unlabeled text.\n",
    "\n",
    "IMDB collects information about movies and lets users write their own reviews, as well as provide a 1-10 numerical rating.  The data for this assignment can be found in hw4_IMDB.csv.  It consists of 12,500 positive and 12,500 negative reviews collected from IMDB.  The ratings have been binarized by labeling anything with score between 7 and 10 as “P” and anything between 1 and 4 with “N” (there are no “neutral” reviews in the data).  We will build and evaluate a system that classifies these movie reviews as positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"hw4_IMDB.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Preprocessing our data\n",
    "\n",
    "Before we build a classifier, we normally do some text pre-processing: text is an unstructured form of data---at least more unstructured than the feature vectors we used in the previous exercises. By pre-processing we can \"clean our textual data\"\n",
    "\n",
    "Do the following preprocessing on the textual data:\n",
    "- Convert all upper case characters to lower case\n",
    "- Remove all non alphanumeric characters\n",
    "- Remove stopwords\n",
    "\n",
    "Why is each of these steps beneficial? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building a Naive Bayes classifier\n",
    "\n",
    "We will now build our predictive model\n",
    "- First, turn the text into a features vector by using 1-grams, also called bag-of-words. This can be done with the CountVectorizer function in SKLearn. What is the shape of the resulting table? What does the number of rows and what do the number of columns mean? \n",
    "- Measure the performance of a Naive Bayes classifier using 3-fold CV, and report the accuract of the classifier across each fold, as well as the average accuracy. Is accuracy a good measure of predictive performance here? If yes, why? If no, what measure would you instead use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Interpreting the results of the classifier\n",
    "\n",
    "Get the cross-validation predictions. Pick some instances that were incorrectly classified and read them through. Are there any words in these misclassified reviews that may have misled the classifier?  Explain with at least three examples for each type of error you can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Improving the performance of your classifier\n",
    "\n",
    "This is an open ended exercise. How far can you push the performance of your classifier? You can try some of the following:\n",
    "- Use 2-grams (ordered pairs of 2 words), or higher degree n-grams\n",
    "- Preproces your data differently. For example, you may choose to not remove some punctuation marks, not lowercasing the words, use a stemmer, or not remove stopwords\n",
    "- Use other predictive algorithms, as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Predicting Yelp star rating\n",
    "\n",
    "Yelp is a very popular website that collects reviews of restaurants and other local businesses, along with star ratings from 1-5. Instead of classifying these reviews as positive or negative, we’ll now build a classifier to predict the star rating of a Yelp review from its text. Star rating prediction can be viewed either as a multiclass classification problem or a regression problem. For now we’ll treat it as multiclass classification. This is our first problem that is not simple binary classification, and will come with its own set of issues, which we will delve into below.\n",
    "\n",
    "### 2.1 Interpreting the new accuracies\n",
    "\n",
    "Read the data from \"hw4_Yelp.csv\" and then perform the preprocessing steps, Naive Bayes classifier fitting, and evaluation as in Part 1.\n",
    "\n",
    "Why are the accuracies lower? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"hw4_Yelp.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Confusion and cost matrices\n",
    "\n",
    "Use the confusion_matrix function from sklearn.metrics to get the confusion matrix for your classifier.\n",
    "\n",
    "We have provided two cost matrices below.  Apply them to your confusion matrix and report the total cost of using this classifier under each cost scheme.  Which one of these (a or b) makes more sense in the context of this multiclass classification problem, and why?\n",
    "\n",
    "**Table 1**\n",
    "\n",
    "|  a  |  b  |  c  |  d  |  e  |     |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "|  0  |  2  |  2  |  2  |  2  | a=1 ||\n",
    "|  2  |  0  |  2  |  2  |  2  | b=2 |\n",
    "|  2  |  2  |  0  |  2  |  2  | c=3 |\n",
    "|  2  |  2  |  2  |  0  |  2  | d=4 |\n",
    "|  2  |  2  |  2  |  2  |  0  | e=5 |\n",
    "\n",
    "**Table 2**\n",
    "\n",
    "|  a  |  b  |  c  |  d  |  e  |     |\n",
    "| --- | --- | --- | --- | --- | --- | \n",
    "|  0  |  1  |  2  |  3  |  4  | a=1 ||\n",
    "|  1  |  0  |  1  |  2  |  3  | b=2 |\n",
    "|  2  |  1  |  0  |  1  |  2  | c=3 |\n",
    "|  3  |  2  |  1  |  0  |  1  | d=4 |\n",
    "|  4  |  3  |  2  |  1  |  0  | e=5 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (bonus question) Using regression\n",
    "\n",
    "Could we instead use a regression predictive model for this problem? Fit a simple linear regression to the training data, using the same pipeline as before. What are some advantages and disadvantages in using a regression instead of multi-class classification  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
