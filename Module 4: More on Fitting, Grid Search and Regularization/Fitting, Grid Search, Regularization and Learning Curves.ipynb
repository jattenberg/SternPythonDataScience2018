{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sudo pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "\n",
    "# some custom libraries!\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ds_utils.decision_surface import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Last Time: Cross validation\n",
    "\n",
    "One of the limitations of model evaluation with a traditional train/test split is \"data loss\". We set aside (say) 20% of our data for evaluation and *never* used it for training. We also never used the 80% of the data set aside for training to test generalizability.  Although this is far better than testing on the training data, which does not measure generalization performance at all, there are a number of potential problems with the simple holdout approach.\n",
    "\n",
    "1) Perhaps the random split was particularly bad (or good).  Do we have any confidence in our accuracy estimate?\n",
    "\n",
    "2) We are using only 20% of the data for testing.  Could we possibly use the data more fully for testing?\n",
    "\n",
    "3) Often we want to know something about the distribution of our evaluation metrics. A simple train/test split only allows a single \"point estimate\"\n",
    "\n",
    "Instead of only making the split once, let's use \"cross-validation\" -- every record will contribute to testing as well as to training.\n",
    "\n",
    "\n",
    "<img src=\"images/cross.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameter Tuning\n",
    "\n",
    "As seen previously, most models have a number of settings that impact their behavior and therefore generalization performance. One common use of cross-validation is to tune these \"hyper-parameters\" (so-called because the coefficients in a model are usually called parameters). One can select several values for a particular hyper-parameter, use cross-validation to estimate model generalization performance, and keep the model that offered the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\"\n",
    "\n",
    "concrete_df = pd.read_excel(concrete_url).dropna()\n",
    "concrete_df.head(5)\n",
    "\n",
    "concrete_df[[\"Concrete compressive strength(MPa, megapascals) \"]].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this into a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_median = concrete_df[\"Concrete compressive strength(MPa, megapascals) \"].median()\n",
    "concrete_df[\"class\"] = concrete_df[\"Concrete compressive strength(MPa, megapascals) \"].apply(\n",
    "        lambda strength: 1 if strength > compression_median else 0\n",
    "    )\n",
    "concrete_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concrete_df.rename(columns={\n",
    "    \"Cement (component 1)(kg in a m^3 mixture)\"             : \"Cement\",\n",
    "    \"Blast Furnace Slag (component 2)(kg in a m^3 mixture)\" : \"Slag\",\n",
    "    \"Fly Ash (component 3)(kg in a m^3 mixture)\"            : \"Fly Ash\",\n",
    "    \"Water  (component 4)(kg in a m^3 mixture)\"             : \"Water\",\n",
    "    \"Superplasticizer (component 5)(kg in a m^3 mixture)\"   : \"Superplasticizer\",\n",
    "    \"Coarse Aggregate  (component 6)(kg in a m^3 mixture)\"  : \"Coarse Agg\",\n",
    "    \"Fine Aggregate (component 7)(kg in a m^3 mixture)\"     : \"Fine Agg\",\n",
    "    \"Age (day)\"                                             : \"Age\",\n",
    "    \"Concrete compressive strength(MPa, megapascals) \"      : \"Strength\"\n",
    "}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_columns = [c for c in concrete_df.columns if c != \"Strength\" and c != \"class\"]\n",
    "predictor_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def score_model(model, df, predictor_columns, class_column, scoring=\"accuracy\"):\n",
    "    scores = cross_val_score(model, df[predictor_columns], df[class_column], scoring=scoring)\n",
    "    return {\"mean\": scores.mean(), \"std_dev\": scores.std()}\n",
    "\n",
    "depths = list(range(2, 50, 3))\n",
    "scores_list = [score_model(DecisionTreeClassifier(max_depth=depth),\n",
    "                           concrete_df,\n",
    "                           predictor_columns,\n",
    "                           \"class\")\n",
    "               for depth in depths]\n",
    "\n",
    "accys = np.array([score[\"mean\"] for score in scores_list])\n",
    "accys_std = np.array([score[\"std_dev\"] for score in scores_list])\n",
    "\n",
    "plt.plot(depths, accys)\n",
    "plt.fill_between(depths, accys + accys_std, accys - accys_std, alpha=0.3)\n",
    "plt.xlabel(\"Tree Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Tuning Tree Depth using Cross Validation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search: Using Cross-Validation to Tune Many Hyper-Parameters\n",
    "\n",
    "The concept explored above can be extended in a very natural way to simultaniuosly optimize many hyper-parameters using a technique called \"grid search\". One first defines a \"grid\", of hyper-parameter values to explore. The grid search explores all possible combinations of these settings, selecting the setting with the best cross-validated value of the chosen generalization measure.\n",
    "\n",
    "Sklearn provides a convenient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {\n",
    "    \"max_depth\": list(range(2, 53, 5)),\n",
    "    \"min_samples_leaf\": list(range(2, 53, 5))\n",
    "}\n",
    "\n",
    "# gridsearchcv behaves just like a model, with fit and predict, with some additional\n",
    "# functionality too\n",
    "tuned_model = GridSearchCV(DecisionTreeClassifier(), grid, scoring=\"accuracy\")\n",
    "tuned_model.fit(concrete_df[predictor_columns], concrete_df[\"class\"])\n",
    "\n",
    "print (\"Best accuracy: %0.3f, using: \" % tuned_model.best_score_)\n",
    "print (tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the performance over our parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "means = tuned_model.cv_results_['mean_test_score']\n",
    "stds = tuned_model.cv_results_['std_test_score']\n",
    "params = tuned_model.cv_results_['params']\n",
    "\n",
    "min_samples_leafs = [param[\"min_samples_leaf\"] for param in params]\n",
    "max_depths = [param[\"max_depth\"] for param in params]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "surf = ax.plot_trisurf(min_samples_leafs, max_depths, means, linewidth=0.1)\n",
    "ax.set_xlabel('min samples in a leaf')\n",
    "ax.set_ylabel('maximum tree depth')\n",
    "ax.set_zlabel('accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Care must be taken when performing grid search: grids with many potential values combined with slow-ish model training can really blow up the time it takes to tune a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "grid = {\n",
    "    \"n_estimators\": list(range(1, 100, 10)),\n",
    "    \"min_samples_leaf\": list(range(2, 200, 20)),\n",
    "    \"criterion\": [\"entropy\", \"gini\"]\n",
    "}\n",
    "\n",
    "# increased verbosity\n",
    "rf_tuned_model = GridSearchCV(RandomForestClassifier(), grid, scoring=\"accuracy\", verbose=1)\n",
    "rf_tuned_model.fit(concrete_df[predictor_columns], concrete_df[\"class\"])\n",
    "\n",
    "print (\"Best accuracy: %0.3f, using: \" % rf_tuned_model.best_score_)\n",
    "print (rf_tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the hyper-parameters tuned above are used to control complexity. This will nearly always be the case when optimizing a ML model-- getting the best generalization performance out of one's model means balancing the tension between allowing a model to fit fine-grained patterns to the data (a good thing), and the tendancy to fit idiosyncracies in a particular data set-- things that don't generalize (a bad thing). \n",
    "\n",
    "## Complexity control for logistic regression (regularization)\n",
    "\n",
    "Recall that when fitting a logistic regression classifier, we try to find the set of weights, $\\textbf{w}$, that maximize the fit to the data, based on some fit (objective) function. In this case, let's call our objective function $g()$, which means that we want $\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w})$.\n",
    "\n",
    "Since we are trying to find the set of weight without too much complexity, when we perform **regularization** we **penalize** our fit as it gets more complex.  This is achieved by adding a \"penalty term\" into the objective function, and using a \"regularization parameter\" $\\lambda$ (also sometimes represented as `c`, which is usually $\\frac{1}{\\lambda}$ so smaller values of `c` lead to larger complexity penalties) to specify how much importance our optimization procedure should place on the fit vs. the penalty:\n",
    "\n",
    "$\\arg\\max_\\textbf{w} g(\\textbf{x}, \\textbf{w}) - \\lambda \\cdot \\text{penalty}(\\textbf{w})$.\n",
    "\n",
    "The two most common type of regularization in logistic regression are the so-called $L_1$ and $L_2$ regularizations, which simply use the sum of the weights (w) and the sum of the squares of the weights, respectively, as the penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Plot different regularization values for L1 and L2 regularization\n",
    "for regularization in ['l2', 'l1']:\n",
    "    \n",
    "    # Print what we are doing\n",
    "    print (\"\\nFitting with %s regularization: \\n\" % regularization)\n",
    "    position = 0\n",
    "\n",
    "    plt.figure(figsize=[15, 21])\n",
    "    c_values = [np.power(10.0, c) for c in range(-6, 3)]\n",
    "    for c in c_values:\n",
    "        position += 1\n",
    "        plt.subplot(3, 3, position)\n",
    "        \n",
    "        model = LogisticRegression(penalty=regularization, C=c)\n",
    "        Decision_Surface(concrete_df[predictor_columns],\n",
    "                         \"Cement\",\n",
    "                         \"Slag\",\n",
    "                         concrete_df[\"class\"],\n",
    "                         model,\n",
    "                         probabilities=True,\n",
    "                         sample=1)\n",
    "        plt.title(\"C=%f\" % c)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature's coefficient weights with L1/L2  and normalization\n",
    "\n",
    "Let's take a look to the different values of our weights with each type of penalty but first, let's understand **normalization**. \n",
    "\n",
    "What is normalization? Why do we need normalization? Each time we work with data, it is very important to consider the \"scale\" of the features. Some features might have distinct values from 1 to 1000, and other features might have values from 0 to 1. As many different data science/machine learning methods compare data along different dimensions, it can often be important to make sure the dimensions are comparable.\n",
    "\n",
    "To do this re-scaling there are are many approaches, the most common being:\n",
    "\n",
    "- _Normalization_ : we rescale our data so that the features have unit norms  \n",
    "- _Standardization_ : we rescale our data acting as if each features is normally distributed (Gaussian with zero mean and unit variance)\n",
    "- _Scaling to a range_ : we rescale our data based on the minimum and maximum value of each feature \n",
    "\n",
    "\n",
    "( sklearn has a built-in function to help us re-scaling our data -- see below)\n",
    "\n",
    "**Let's take a look at the data before and after re-scaling.**\n",
    "\n",
    "Before re-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "concrete_df[predictor_columns] = scaler.fit_transform(concrete_df[predictor_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try and delve into the impact regularization has on the coefficients logistic regression assigns to different features, let's investigate the coefficients before any regularization is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_coeffs(df, lr, predictor_columns, class_column):\n",
    "    lr.fit(df[predictor_columns], df[class_column])\n",
    "\n",
    "    return dict(zip(predictor_columns, lr.coef_[0]))\n",
    "\n",
    "coefs = get_lr_coeffs(concrete_df, LogisticRegression(), predictor_columns, \"class\")\n",
    "pd.DataFrame([coefs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how the model weights change with differing degrees of complexity control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lr_regularization_paths(df, predictor_columns, class_column, regtype, reg_values):\n",
    "    coefs = [get_lr_coeffs(concrete_df,\n",
    "                          LogisticRegression(penalty=regtype, C=10**reg),\n",
    "                          predictor_columns,\n",
    "                          class_column)\n",
    "             for reg in reg_values]\n",
    "\n",
    "    df = pd.DataFrame(coefs)\n",
    "    df[\"regularization\"] = reg_values\n",
    "    \n",
    "    df.set_index(\"regularization\", inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = np.arange(-5, 5, 0.5)  #go through a bunch of ascending regularization parameters\n",
    "    \n",
    "l1_coefs = get_lr_regularization_paths(concrete_df,\n",
    "                                       predictor_columns,\n",
    "                                       \"class\",\n",
    "                                       \"l1\",\n",
    "                                       regs)\n",
    "\n",
    "l2_coefs = get_lr_regularization_paths(concrete_df,\n",
    "                                       predictor_columns,\n",
    "                                       \"class\",\n",
    "                                       \"l2\",\n",
    "                                       regs)\n",
    "\n",
    "l1_coefs.plot()\n",
    "plt.title(\"L1 Regularization paths\")\n",
    "\n",
    "l2_coefs.plot()\n",
    "plt.title(\"L2 Regularization paths\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "(Briefly covered in the last class)\n",
    "\n",
    "Throughout the past two lectures we analyzed and compared model accuracies using a unique **sample size** that was essentially fixed and determined by the size of the toy datasets we are considering. \n",
    "\n",
    "Very often we also want to assess the relationship between how much data we are using to train the models, and the generalization performance we achieve.  For example, do we have a good idea whether we should invest in acquiring more training data? The only way to answer this question is again, experiment with different sample sizes. The main way to do this assessment is via **_learning curves_**: analyze the change of the generalization performance (accuray on the holdout data, in this case) based on different sizes of the training set.\n",
    "\n",
    "What would we expect to see? Holding everything else fixed, the generalization should be better with more training data, up until a certain point. Then, more data won't increase generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection as cv\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model_on_sample(df, model, predictor_cols, class_col, pct, scoring=accuracy_score):\n",
    "    kf = cv.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    X = df[predictor_cols]\n",
    "    y = df[class_col]\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, y):  \n",
    "        # only take a portion of the training\n",
    "        sampled_indices = np.random.permutation(range(len(train_index)))[:int(pct*len(train_index))].tolist()\n",
    "        np_train = np.array(train_index)\n",
    "        to_get = np_train[sampled_indices]\n",
    "\n",
    "        model.fit(X.loc[to_get], y[to_get])\n",
    "        scores.append(scoring(y[test_index], model.predict(X.loc[test_index])))\n",
    "        \n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pcts = np.linspace(0.01,1,100).tolist()\n",
    "dt_scores = [evaluate_model_on_sample(concrete_df,\n",
    "                                     DecisionTreeClassifier(),\n",
    "                                     predictor_columns,\n",
    "                                     \"class\",\n",
    "                                     pct)\n",
    "             for pct in pcts]\n",
    "\n",
    "lr_scores = [evaluate_model_on_sample(concrete_df,\n",
    "                                      LogisticRegression(),\n",
    "                                      predictor_columns,\n",
    "                                      \"class\",\n",
    "                                      pct)\n",
    "             for pct in pcts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dt_score = np.array([s[0] for s in dt_scores])\n",
    "std_dt_score = np.array([s[1] for s in dt_scores])\n",
    "\n",
    "raw_lr_score = np.array([s[0] for s in lr_scores])\n",
    "std_lr_score = np.array([s[1] for s in lr_scores])\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(pcts, raw_dt_score, label=\"Classifier Tree\")\n",
    "plt.plot(pcts, raw_lr_score, label=\"Logistic Regression\")\n",
    "plt.xlabel(\"Percent of data\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pcts, raw_dt_score, label=\"Classifier Tree\")\n",
    "plt.fill_between(pcts, raw_dt_score + std_dt_score, raw_dt_score - std_dt_score, alpha=0.3)\n",
    "plt.plot(pcts, raw_lr_score, label=\"Logistic Regression\")\n",
    "plt.fill_between(pcts, raw_lr_score + std_lr_score, raw_lr_score - std_lr_score, alpha=0.3)\n",
    "plt.xlabel(\"Percent of data\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, learning curves will help to determine at least 2 things:\n",
    "\n",
    "- We can see which model performs better or worse for each sample size (e.g. Decision Tree vs Logistic Regression)\n",
    "- We can get a sense of whether getting more data (or using less) will improve (or not degrade) generalization.\n",
    "\n",
    "We can also think of ways to add more complexity to a predictive model. Here, we build a model that considers polynomial combinations of features (as in our last class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
